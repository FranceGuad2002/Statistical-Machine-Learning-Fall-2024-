{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48dc4b5f-0bb8-4e50-acb4-707e1f130880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy as sp\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18250118-a4f5-434b-ab4e-4ba661880e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency(length):\n",
    "    # now we want to create a couple of graphs. In practice, we start setting the number of nodes for each graph.\n",
    "\n",
    "\n",
    "    # for each couple of nodes in a graph we generate a random natural number between 0 and 100.\n",
    "\n",
    "    matrix = np.zeros((length, length))\n",
    "\n",
    "\n",
    "\n",
    "    # we also want our graphs to be representative of reality (i.e. graphs are usually sparse).\n",
    "\n",
    "    for i in range(length):\n",
    "        for j in range(i):\n",
    "            rnd = np.random.randint(1, 101)\n",
    "            unif = np.random.rand()\n",
    "            if unif > 0.6:\n",
    "                matrix[i][j] = rnd\n",
    "                matrix[j][i] = rnd\n",
    "            else:\n",
    "                matrix[i][j] = 0\n",
    "                matrix[j][i] = 0\n",
    "\n",
    "\n",
    "    return matrix\n",
    "\n",
    "    # each row is a node, for a fixed row each column is another node, and the number is the edge's values.\n",
    "\n",
    "def build_measure(matrix):\n",
    "\n",
    "\n",
    "    # now we want to give a weight to each node in each matrix.\n",
    "    # in particulare if a node has a lot of connections, it should have a weight which is linear wrt the sum of its edges.\n",
    "\n",
    "    M = []\n",
    "    length = matrix.shape[0]\n",
    "\n",
    "    for i in range(length):\n",
    "        s = sum(matrix[i,:])\n",
    "        M.append(s)\n",
    "    M = M/sum(M)\n",
    "\n",
    "   \n",
    "    return M\n",
    "\n",
    "def build_similitude_matrices(matrix):    \n",
    "   \n",
    "    # this are matrices which represent the similitude between nodes.\n",
    "\n",
    "    C = 1/(1 + matrix)\n",
    "\n",
    "    return C\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_noise(matrix, noise_level=0.0001):\n",
    "    \"\"\"\n",
    "    Introduces noise into the adjacency matrix.\n",
    "   \n",
    "    Args:\n",
    "        matrix (numpy.ndarray): The original adjacency matrix.\n",
    "        noise_level (float): The probability of modifying an existing edge or adding a new one (default 0.1).\n",
    "   \n",
    "    Returns:\n",
    "        numpy.ndarray: The new adjacency matrix with added noise.\n",
    "    \"\"\"\n",
    "    # creare a copy of the matrix to avoid altering the original.\n",
    "    noisy_matrix = matrix.copy()\n",
    "    length = noisy_matrix.shape[0]\n",
    "   \n",
    "    # iterate over the lower triangular part of the matrix (to maintain symmetry).\n",
    "    for i in range(length):\n",
    "        for j in range(i):\n",
    "            unif = np.random.rand()\n",
    "           \n",
    "            if noisy_matrix[i][j] > 0:  # if there is already an edge.\n",
    "                # with probability `noise_level`, change the edge's value.\n",
    "                if unif < noise_level:\n",
    "                    noisy_matrix[i][j] = np.random.randint(1, 101)\n",
    "                    noisy_matrix[j][i] = noisy_matrix[i][j]  # ensure symmetry.\n",
    "            else:  # if there is no edge.\n",
    "                # with probability `noise_level`, add a new edge.\n",
    "                if unif < noise_level:\n",
    "                    rnd = np.random.randint(1, 101)\n",
    "                    noisy_matrix[i][j] = rnd\n",
    "                    noisy_matrix[j][i] = rnd  # ensure symmetry.\n",
    "   \n",
    "    return noisy_matrix\n",
    "\n",
    "def print_embeddings_one(embed_s, embed_t):\n",
    "\n",
    "    matrix = embed_s\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = matrix[0, :]\n",
    "    y = matrix[1, :]\n",
    "    z = matrix[2, :]\n",
    "\n",
    "    ax.scatter(x, y, z, color='r',marker='o')\n",
    "\n",
    "    x2=embed_t[0,:]\n",
    "    y2 = embed_t[1,:]\n",
    "    z2 = embed_t[2,:]\n",
    "    ax.scatter(x2,y2,z2, color='b',marker='o')\n",
    "\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    ax.set_title('plot')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def create_embeddings_animation(embed_s_history, embed_t_history, output_path=\"embedding_animation.mp4\", fps=2):\n",
    "    \"\"\"\n",
    "    Creates an animation of embeddings over iterations, dynamically adjusting the view.\n",
    "\n",
    "    Parameters:\n",
    "    - embed_s_history (list of np.ndarray): List of `embed_s` embeddings (one for each iteration).\n",
    "    - embed_t_history (list of np.ndarray): List of `embed_t` embeddings (one for each iteration).\n",
    "    - output_path (str): Path to save the animation file (e.g., .mp4 or .gif).\n",
    "    - fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "    # creare a 3D figure.\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # initialize empty scatter plots for source and target embeddings.\n",
    "    scatter_s = ax.scatter([], [], [], color='r', marker='o', label='Source')\n",
    "    scatter_t = ax.scatter([], [], [], color='b', marker='o', label='Target')\n",
    "\n",
    "    # add a legend.\n",
    "    ax.legend()\n",
    "    title = ax.set_title(\"Embedding Evolution\")\n",
    "\n",
    "    def update(frame):\n",
    "        \"\"\"\n",
    "        Update the scatter plot and axes limits for each animation frame.\n",
    "        \"\"\"\n",
    "        # get embeddings for the current frame.\n",
    "        embed_s = embed_s_history[frame]\n",
    "        embed_t = embed_t_history[frame]\n",
    "\n",
    "        # dynamically calculate axis limits based on current embeddings.\n",
    "        all_points = np.hstack([embed_s, embed_t])\n",
    "        x_min, x_max = all_points[0, :].min() - 1, all_points[0, :].max() + 1\n",
    "        y_min, y_max = all_points[1, :].min() - 1, all_points[1, :].max() + 1\n",
    "        z_min, z_max = all_points[2, :].min() - 1, all_points[2, :].max() + 1\n",
    "\n",
    "        ax.set_xlim([x_min, x_max])\n",
    "        ax.set_ylim([y_min, y_max])\n",
    "        ax.set_zlim([z_min, z_max])\n",
    "\n",
    "        # update the scatter plots with the current embeddings.\n",
    "        scatter_s._offsets3d = (embed_s[0, :], embed_s[1, :], embed_s[2, :])\n",
    "        scatter_t._offsets3d = (embed_t[0, :], embed_t[1, :], embed_t[2, :])\n",
    "\n",
    "        # update the title to reflect the current iteration.\n",
    "        title.set_text(f\"Embedding Evolution - Iteration {frame + 1}\")\n",
    "\n",
    "        return scatter_s, scatter_t, title\n",
    "\n",
    "    # create the animation.\n",
    "    ani = FuncAnimation(fig, update, frames=len(embed_s_history), interval=1000 // fps, blit=False)\n",
    "\n",
    "    # save the animation as an MP4 or GIF file.\n",
    "    writer = FFMpegWriter(fps=fps, metadata=dict(artist='Me'), bitrate=1800)\n",
    "    ani.save(output_path, writer=writer)\n",
    "\n",
    "    # close the figure.\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def create_embeddings_animation_2(embed_s_history, embed_t_history, output_path=\"embedding_animation.mp4\", fps=2):\n",
    "    \"\"\"\n",
    "    Creates an animation of embeddings over iterations, dynamically adjusting the view and adding fixed labels.\n",
    "\n",
    "    Parameters:\n",
    "    - embed_s_history (list of np.ndarray): List of `embed_s` embeddings (one for each iteration).\n",
    "    - embed_t_history (list of np.ndarray): List of `embed_t` embeddings (one for each iteration).\n",
    "    - output_path (str): Path to save the animation file (e.g., .mp4 or .gif).\n",
    "    - fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "    # creare una figura 3D.\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # inizializzare scatter plot vuoti.\n",
    "    scatter_s = ax.scatter([], [], [], color='r', marker='o', label='Source')\n",
    "    scatter_t = ax.scatter([], [], [], color='b', marker='o', label='Target')\n",
    "\n",
    "    # aggiungere legenda.\n",
    "    ax.legend()\n",
    "    title = ax.set_title(\"Embedding Evolution\")\n",
    "\n",
    "    # lista per salvare le etichette.\n",
    "    text_labels_s = []\n",
    "    text_labels_t = []\n",
    "\n",
    "    def update(frame):\n",
    "        \"\"\"\n",
    "        Update scatter plot, axis limits, and labels for each animation frame.\n",
    "        \"\"\"\n",
    "        nonlocal text_labels_s, text_labels_t  # refer to the outer lists.\n",
    "\n",
    "        # cancellare le etichette precedenti.\n",
    "        for text in text_labels_s + text_labels_t:\n",
    "            text.remove()\n",
    "        text_labels_s = []\n",
    "        text_labels_t = []\n",
    "\n",
    "        # ottenere embeddings per il frame corrente.\n",
    "        embed_s = embed_s_history[frame]\n",
    "        embed_t = embed_t_history[frame]\n",
    "\n",
    "        # calcolare dinamicamente i limiti degli assi.\n",
    "        all_points = np.hstack([embed_s, embed_t])\n",
    "        x_min, x_max = all_points[0, :].min() - 1, all_points[0, :].max() + 1\n",
    "        y_min, y_max = all_points[1, :].min() - 1, all_points[1, :].max() + 1\n",
    "        z_min, z_max = all_points[2, :].min() - 1, all_points[2, :].max() + 1\n",
    "\n",
    "        ax.set_xlim([x_min, x_max])\n",
    "        ax.set_ylim([y_min, y_max])\n",
    "        ax.set_zlim([z_min, z_max])\n",
    "\n",
    "        # aggiornare scatter plot con embeddings correnti.\n",
    "        scatter_s._offsets3d = (embed_s[0, :], embed_s[1, :], embed_s[2, :])\n",
    "        scatter_t._offsets3d = (embed_t[0, :], embed_t[1, :], embed_t[2, :])\n",
    "\n",
    "        # aggiungere etichette ai punti (con numeri fissi).\n",
    "        for i in range(embed_s.shape[1]):\n",
    "            text_labels_s.append(\n",
    "                ax.text(embed_s[0, i], embed_s[1, i], embed_s[2, i], f\"{i}\", color='red', fontsize=8)\n",
    "            )\n",
    "        for i in range(embed_t.shape[1]):\n",
    "            text_labels_t.append(\n",
    "                ax.text(embed_t[0, i], embed_t[1, i], embed_t[2, i], f\"{i}\", color='blue', fontsize=8)\n",
    "            )\n",
    "\n",
    "        # aggiornare il titolo per riflettere l'iterazione corrente.\n",
    "        title.set_text(f\"Embedding Evolution - Iteration {frame + 1}\")\n",
    "\n",
    "        return scatter_s, scatter_t, title, *text_labels_s, *text_labels_t\n",
    "\n",
    "    # creare l'animazione.\n",
    "    anim = FuncAnimation(fig, update, frames=len(embed_s_history), interval=1000 / fps)\n",
    "\n",
    "    # salvare l'animazione in un file.\n",
    "    anim.save(output_path, writer='ffmpeg', fps=fps)\n",
    "    print(f\"Animation saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GWL_simple_2(C_s, C_t, M_s, M_t, hyper, noise):\n",
    "    \"\"\"\n",
    "    Simplified Gromov-Wasserstein Learning (GWL) with entropy-regularized cost, embedding updates,\n",
    "    and regularization R(X_s, X_t).\n",
    "   \n",
    "    Parameters:\n",
    "    - C_s: Cost matrix for source graph (length_s x length_s).\n",
    "    - C_t: Cost matrix for target graph (length_t x length_t).\n",
    "    - M_s: Marginal distribution for source graph (length_s,).\n",
    "    - M_t: Marginal distribution for target graph (length_t,).\n",
    "    - beta: Regularization parameter for Sinkhorn iterations.\n",
    "    - gamma: Regularization parameter for entropy term.\n",
    "    - alpha: Weight for embedding-level cost regularization.\n",
    "    - d: Dimension of the embeddings.\n",
    "    - inner_n: Number of inner Sinkhorn iterations.\n",
    "    - outer_n: Number of outer embedding updates.\n",
    "    - lambda_reg: Regularization weight for R(X_s, X_t).\n",
    "    \"\"\"\n",
    "\n",
    "    beta = hyper[\"beta\"]\n",
    "    gamma = hyper[\"gamma\"]\n",
    "    d = hyper[\"d\"]\n",
    "    inner_n = hyper[\"inner_n\"]\n",
    "    outer_n = hyper[\"outer_n\"]\n",
    "    lambda_reg = hyper[\"lambda_reg\"]\n",
    "\n",
    "   \n",
    "    length_s, length_t = C_s.shape[0], C_t.shape[0]\n",
    "\n",
    "    # initialize embeddings.\n",
    "    embed_s = np.random.rand(d, length_s) * 5\n",
    "    # embed_t = np.random.rand(d, length_t) * 5\n",
    "    noise = np.random.normal(loc=0, scale = noise, size=length_s)\n",
    "    embed_t = embed_s + noise\n",
    "    print(embed_s, \"AAA \\n \\n\", embed_t)\n",
    "    # for the animation.\n",
    "    embed_s_history = []\n",
    "    embed_t_history = []\n",
    "\n",
    "\n",
    "    # initialize transport matrix.\n",
    "    T = np.outer(M_s, M_t)\n",
    "\n",
    "    for m in range(outer_n):\n",
    "        alpha_m = m / outer_n  # gradual interpolation weight.\n",
    "\n",
    "        embed_s_history.append(embed_s.copy())\n",
    "        embed_t_history.append(embed_t.copy())\n",
    "\n",
    "        # inner loop: pdate transport matrix T using entropy regularization.\n",
    "        for n in range(inner_n):\n",
    "            # compute embedding-level cost.\n",
    "            K_embed = np.zeros((length_s, length_t))\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_t):\n",
    "                    # if L2.\n",
    "                    K_embed[i, j] = np.sum((embed_s[:, i] - embed_t[:, j]) ** 2)\n",
    "                    # if cosine.\n",
    "                    # K_embed [i,j]= 1 - np.exp(-10*(1 - np.dot(embed_s[:, i], embed_t[:,j])/(np.linalg.norm(embed_s[:,i])*np.linalg.norm(embed_t[:,j]))))\n",
    "\n",
    "\n",
    "            # compute graph-level cost.\n",
    "            graph_cost = C_s @ T @ C_t.T\n",
    "            C_mn = graph_cost + alpha_m * K_embed + gamma\n",
    "\n",
    "            # add entropy regularization term.\n",
    "            C_reg = C_mn - gamma * np.log(T + 1e-8)\n",
    "           \n",
    "            # sinkhorn update.\n",
    "            kernel = np.exp(-C_reg / beta)\n",
    "            a = np.ones(length_s)\n",
    "            for _ in range(inner_n):\n",
    "                b = M_t / (kernel.T @ a)\n",
    "                a = M_s / (kernel @ b)\n",
    "\n",
    "            T = np.diag(a) @ kernel @ np.diag(b)\n",
    "\n",
    "        # update embeddings by minimizing α_m <K(X_s, X_t), T> + β R(X_s, X_t).\n",
    "        for _ in range(10):  # gradient descent steps.\n",
    "            # compute gradients of K(X_s, X_t).\n",
    "            grad_s = np.zeros_like(embed_s)\n",
    "            grad_t = np.zeros_like(embed_t)\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_t):\n",
    "                    grad_s[:, i] += 2 * T[i, j] * (embed_s[:, i] - embed_t[:, j])\n",
    "                    grad_t[:, j] += 2 * T[i, j] * (embed_t[:, j] - embed_s[:, i])\n",
    "\n",
    "            # add gradients of R(X_s, X_t).\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_s):\n",
    "                    grad_s[:, i] += lambda_reg * 4 * ((np.sum((embed_s[:, i] - embed_s[:, j]) ** 2) - C_s[i, j]) *\n",
    "                                                      (embed_s[:, i] - embed_s[:, j]))\n",
    "\n",
    "            for i in range(length_t):\n",
    "                for j in range(length_t):\n",
    "                    grad_t[:, i] += lambda_reg * 4 * ((np.sum((embed_t[:, i] - embed_t[:, j]) ** 2) - C_t[i, j]) *\n",
    "                                                      (embed_t[:, i] - embed_t[:, j]))\n",
    "\n",
    "            # gradient descent step.\n",
    "            embed_s -= alpha_m * grad_s\n",
    "            embed_t -= alpha_m * grad_t\n",
    "\n",
    "        # # #print_embeddings_one(embed_s, embed_t)\n",
    "\n",
    "    # compute final loss.\n",
    "    final_cost = np.sum(T * C_mn)\n",
    "    entropy = -np.sum(T * np.log(T + 1e-8))\n",
    "    total_loss = final_cost - gamma * entropy\n",
    "\n",
    "\n",
    "    # graph matching.\n",
    "    correspondence = np.zeros_like(T)\n",
    "    for i in range(length_s):\n",
    "        max_idx = np.argmax(T[i, :])\n",
    "        correspondence[i, max_idx] = 1\n",
    "\n",
    "\n",
    "    # print_embeddings_video(embed_s, embed_t)\n",
    "   \n",
    "\n",
    "    return embed_s, embed_t, T, correspondence, total_loss, embed_s_history, embed_t_history\n",
    "\n",
    "\n",
    "\n",
    "def build_barabasi_sp(n):\n",
    "    G = nx.barabasi_albert_graph(n,1,seed = 43)\n",
    "    SP= dict(nx.shortest_path_length(G))\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u != v :\n",
    "                G.add_edge(u,v,weight = 1 / (SP[u][v] + 1))\n",
    "    m= nx.adjacency_matrix(G).todense()\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "def build_barabasi_random(n):\n",
    "    G = nx.barabasi_albert_graph(n,1,seed = 43)\n",
    "    for (u,v) in G.edges():\n",
    "        w = np.random.randint(0,5)\n",
    "        G.edges()[u,v][\"weight\"] = w\n",
    "    m= nx.adjacency_matrix(G).todense()\n",
    "    return m\n",
    "\n",
    "\n",
    "def makenoise_2(m, I, A, delta):\n",
    "    m = np.array(m)\n",
    "    G = nx.from_numpy_array(m)\n",
    "    G2 = nx.Graph()\n",
    "    G2.add_nodes_from(G.nodes())\n",
    "    G2.add_edges_from(G.edges())\n",
    "    l = len(G.nodes())\n",
    "    K = len(G.edges())\n",
    "    for i in range(l):\n",
    "        for j in range(i):\n",
    "            w = np.random.randint(0,I)\n",
    "            if (i,j) in G.edges():\n",
    "                G.edges()[(i,j)][\"weight\"] = G.edges()[(i,j)][\"weight\"]+w\n",
    "\n",
    "    surplus = int(l* delta)\n",
    "    for i in range(surplus):\n",
    "        G2.add_node(l+i)\n",
    "        for a in range(int(K/l)):\n",
    "            k = np.random.randint(0,l-1)\n",
    "            w = np.random.randint(0,A)\n",
    "            G2.add_edge(l+i,k,weight=w)\n",
    "\n",
    "    m2 = nx.adjacency_matrix(G2)\n",
    "\n",
    "    return m2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "def hyperparameter_optimization(C_s, C_t, M_s, M_t, hyper_ranges, noise):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization using a grid search.\n",
    "\n",
    "    Args:\n",
    "        C_s (ndarray): Source cost matrix.\n",
    "        C_t (ndarray): Target cost matrix.\n",
    "        M_s (ndarray): Source marginal distribution.\n",
    "        M_t (ndarray): Target marginal distribution.\n",
    "        hyper_ranges (dict): Ranges for the hyperparameters to optimize.\n",
    "            Example: {\"beta\": [1, 10, 100], \"gamma\": [0.1, 1, 10], \"d\": [2, 3, 5]}\n",
    "        noise (float): Noise level for embeddings.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameters and their corresponding total loss.\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    best_hyperparams = {}\n",
    "    results = []\n",
    "\n",
    "    # Generate all combinations of hyperparameter values\n",
    "    keys, values = zip(*hyper_ranges.items())\n",
    "    combinations = list(product(*values))\n",
    "\n",
    "    for combination in combinations:\n",
    "        # Map the combination to hyperparameter names\n",
    "        hyper = dict(zip(keys, combination))\n",
    "\n",
    "        # Perform GWL with the current hyperparameters\n",
    "        _, _, _, _, total_loss, _, _ = GWL_simple_2(C_s, C_t, M_s, M_t, hyper, noise)\n",
    "\n",
    "        # Save the result\n",
    "        results.append({\"hyperparameters\": hyper, \"loss\": total_loss})\n",
    "\n",
    "        # Update the best hyperparameters if current loss is lower\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_hyperparams = hyper\n",
    "\n",
    "        print(f\"Tested hyperparameters: {hyper}, Total loss: {total_loss}\")\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}, Best loss: {best_loss}\")\n",
    "    return {\"best_hyperparameters\": best_hyperparams, \"best_loss\": best_loss, \"results\": results}\n",
    "def adj_matrix_sp(G):\n",
    "    SP = dict(nx.shortest_path_length(G))\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if ( u != v):\n",
    "                G.add_edge(u,v,weight = 1/(SP[u][v]+1))\n",
    "    m = nx.adjacency_matrix(G).todense()\n",
    "    return m\n",
    "\n",
    "def noise_sp(G, q, s):\n",
    "    # The graph G SHOULD BE ONLY TOPOLOGICAL, NO WEIGHTS\n",
    "    # q is the noise\n",
    "    # s number of edges to every node that I add\n",
    "    G2 = nx.Graph()\n",
    "    G2.add_nodes_from(G.nodes())\n",
    "    G2.add_edges_from(G.edges())\n",
    "    l = len(G.nodes())\n",
    "    L = len(G.edges())\n",
    "    surplus = int(l*q)\n",
    "    for i in range(surplus):\n",
    "        G2.add_node(l+i)\n",
    "        for a in range(int(s)):\n",
    "            k = np.random.randint(0,l)\n",
    "            G2.add_edge(l+i,k)\n",
    "\n",
    "    return G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00766367-0972-4fe1-a756-cc6b82d8f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GWL_simple_3(C_s, C_t, M_s, M_t, emb_s , emb_t, hyper):\n",
    "    \"\"\"\n",
    "    Simplified Gromov-Wasserstein Learning (GWL) with entropy-regularized cost, embedding updates,\n",
    "    and regularization R(X_s, X_t).\n",
    "   \n",
    "    Parameters:\n",
    "    - C_s: Cost matrix for source graph (length_s x length_s).\n",
    "    - C_t: Cost matrix for target graph (length_t x length_t).\n",
    "    - M_s: Marginal distribution for source graph (length_s,).\n",
    "    - M_t: Marginal distribution for target graph (length_t,).\n",
    "    - beta: Regularization parameter for Sinkhorn iterations.\n",
    "    - gamma: Regularization parameter for entropy term.\n",
    "    - alpha: Weight for embedding-level cost regularization.\n",
    "    - d: Dimension of the embeddings.\n",
    "    - inner_n: Number of inner Sinkhorn iterations.\n",
    "    - outer_n: Number of outer embedding updates.\n",
    "    - lambda_reg: Regularization weight for R(X_s, X_t).\n",
    "    \"\"\"\n",
    "\n",
    "    beta = hyper[\"beta\"]\n",
    "    gamma = hyper[\"gamma\"]\n",
    "    d = hyper[\"d\"]\n",
    "    inner_n = hyper[\"inner_n\"]\n",
    "    outer_n = hyper[\"outer_n\"]\n",
    "    lambda_reg = hyper[\"lambda_reg\"]\n",
    "\n",
    "   \n",
    "    length_s, length_t = C_s.shape[0], C_t.shape[0]\n",
    "\n",
    "    # initialize embeddings.\n",
    "    embed_s = emb_s\n",
    "    # embed_t = np.random.rand(d, length_t) * 5\n",
    "    #noise = np.random.normal(loc=0, scale = noise, size=length_s)\n",
    "    embed_t = emb_t\n",
    "    # for the animation.\n",
    "    embed_s_history = []\n",
    "    embed_t_history = []\n",
    "\n",
    "\n",
    "    # initialize transport matrix.\n",
    "    T = np.outer(M_s, M_t)\n",
    "\n",
    "    for m in range(outer_n):\n",
    "        alpha_m = m / outer_n  # gradual interpolation weight.\n",
    "\n",
    "        embed_s_history.append(embed_s.copy())\n",
    "        embed_t_history.append(embed_t.copy())\n",
    "\n",
    "        # inner loop: pdate transport matrix T using entropy regularization.\n",
    "        for n in range(inner_n):\n",
    "            # compute embedding-level cost.\n",
    "            K_embed = np.zeros((length_s, length_t))\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_t):\n",
    "                    # if L2.\n",
    "                    K_embed[i, j] = np.sum((embed_s[:, i] - embed_t[:, j]) ** 2)\n",
    "                    # if cosine.\n",
    "                    # K_embed [i,j]= 1 - np.exp(-10*(1 - np.dot(embed_s[:, i], embed_t[:,j])/(np.linalg.norm(embed_s[:,i])*np.linalg.norm(embed_t[:,j]))))\n",
    "\n",
    "\n",
    "            # compute graph-level cost.\n",
    "            graph_cost = C_s @ T @ C_t.T\n",
    "            C_mn = graph_cost + alpha_m * K_embed + gamma\n",
    "\n",
    "            # add entropy regularization term.\n",
    "            C_reg = C_mn - gamma * np.log(T + 1e-8)\n",
    "           \n",
    "            # sinkhorn update.\n",
    "            kernel = np.exp(-C_reg / beta)\n",
    "            a = np.ones(length_s)\n",
    "            for _ in range(inner_n):\n",
    "                b = M_t / (kernel.T @ a)\n",
    "                a = M_s / (kernel @ b)\n",
    "\n",
    "            T = np.diag(a) @ kernel @ np.diag(b)\n",
    "\n",
    "        # update embeddings by minimizing α_m <K(X_s, X_t), T> + β R(X_s, X_t).\n",
    "        for _ in range(10):  # gradient descent steps.\n",
    "            # compute gradients of K(X_s, X_t).\n",
    "            grad_s = np.zeros_like(embed_s)\n",
    "            grad_t = np.zeros_like(embed_t)\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_t):\n",
    "                    grad_s[:, i] += 2 * T[i, j] * (embed_s[:, i] - embed_t[:, j])\n",
    "                    grad_t[:, j] += 2 * T[i, j] * (embed_t[:, j] - embed_s[:, i])\n",
    "\n",
    "            # add gradients of R(X_s, X_t).\n",
    "            for i in range(length_s):\n",
    "                for j in range(length_s):\n",
    "                    grad_s[:, i] += lambda_reg * 4 * ((np.sum((embed_s[:, i] - embed_s[:, j]) ** 2) - C_s[i, j]) *\n",
    "                                                      (embed_s[:, i] - embed_s[:, j]))\n",
    "\n",
    "            for i in range(length_t):\n",
    "                for j in range(length_t):\n",
    "                    grad_t[:, i] += lambda_reg * 4 * ((np.sum((embed_t[:, i] - embed_t[:, j]) ** 2) - C_t[i, j]) *\n",
    "                                                      (embed_t[:, i] - embed_t[:, j]))\n",
    "\n",
    "            # gradient descent step.\n",
    "            embed_s -= alpha_m * grad_s\n",
    "            embed_t -= alpha_m * grad_t\n",
    "\n",
    "        # # #print_embeddings_one(embed_s, embed_t)\n",
    "\n",
    "    # compute final loss.\n",
    "    final_cost = np.sum(T * C_mn)\n",
    "    entropy = -np.sum(T * np.log(T + 1e-8))\n",
    "    total_loss = final_cost - gamma * entropy\n",
    "\n",
    "\n",
    "    # graph matching.\n",
    "    correspondence = np.zeros_like(T)\n",
    "    for i in range(length_s):\n",
    "        max_idx = np.argmax(T[i, :])\n",
    "        correspondence[i, max_idx] = 1\n",
    "\n",
    "\n",
    "    # print_embeddings_video(embed_s, embed_t)\n",
    "   \n",
    "\n",
    "    return embed_s, embed_t, T, correspondence, total_loss, embed_s_history, embed_t_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca17ff0-a2e2-4c36-a1d5-f0b2d36dd040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee063b3-b949-401a-8746-735d4ff6c5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
